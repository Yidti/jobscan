{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1d4b84-ce72-4871-8366-dc78a7fb17fb",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50a30a2-e1b7-40cc-af24-427c18426ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, time, requests, random\n",
    "from datetime import datetime\n",
    "# 爬蟲\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import grequests # 看起要在.py才能用\n",
    "\n",
    "# 非同步HTTP請求\n",
    "from aiohttp import ClientSession, TCPConnector\n",
    "# Python標準庫中提供的用於支援異步編程的模組, 可使用async和await關鍵字來定義異步協程\n",
    "import asyncio\n",
    "\n",
    "# 處理嵌套事件迴圈（nested event loop）的庫, 在Python中，通常只能有一個事件迴圈運行，\n",
    "# 但某些情況下，比如在Jupyter Notebook中執行異步代碼時，可能會遇到嵌套事件迴圈的問題。\n",
    "# nest_asyncio的作用就是解決這個問題，允許在已有事件迴圈的情況下再創建一個新的事件迴圈。\n",
    "import nest_asyncio\n",
    "# 它會修改當前執行環境，允許在已有事件迴圈的情況下再次建立一個新的事件迴圈，\n",
    "# 通常用於處理一些特定的情況，例如在Jupyter Notebook中執行異步代碼。\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2ac55-a1a1-43d7-b10c-7b0a45d15125",
   "metadata": {},
   "source": [
    "# reload之後再執行 網路爬蟲 Web crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7582c23b-cd00-4645-99c3-49cd4208a305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.104.com.tw/jobs/search/?ro=1&keyword=%E5%BE%8C%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%AB+python&area=6001002000%2C6001001000%2C6001005000%2C6001008000&isnew=3&jobexp=1%2C3&mode=l&order=16&asc=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████| 35/35 [01:01<00:00,  1.75s/page]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入35頁 | 載入1030筆資料 | 過濾後剩778筆資料 | 花費 77.97 秒\n"
     ]
    }
   ],
   "source": [
    "from crawler104 import Crawler104\n",
    "import async_job\n",
    "from config.search_params import get_filter_params\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "keywords_pattern = r'工程|資料|python|data|數據|後端'\n",
    "\n",
    "# custom filter search\n",
    "role = '全職'\n",
    "keyword = \"後端工程師 python\"\n",
    "cities = ['新北市', '台北市', '桃園市', '台中市']\n",
    "isnew = '三日內'\n",
    "jobexp = ['1年以下', '1-3年']\n",
    "model = '列表'  # 一次能呈現比較多筆資料\n",
    "order = '日期排序'\n",
    "asc = '遞減'\n",
    "\n",
    "filter_params = get_filter_params(role, keyword, cities, isnew, jobexp, model, order, asc)\n",
    "\n",
    "# 碼表 Start\n",
    "start_time = time.time()\n",
    "# 建立df物件\n",
    "df = pd.DataFrame()\n",
    "crawler104 = Crawler104(filter_params, keywords_pattern)\n",
    "# 開始爬蟲\n",
    "raw_jobs_104 = crawler104.search_job()\n",
    "filter_jobs_104 = crawler104.filter_job(raw_jobs_104)\n",
    "# result_df = js104.main(filter_jobs, df)\n",
    "# 碼表 End\n",
    "print(f\"花費 {np.round((time.time() - start_time),2)} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9220c210-4531-434d-beb7-5d1edd8d79cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config.search_params' from 'C:\\\\Users\\\\Rekam\\\\Documents\\\\Jobscan\\\\config\\\\search_params.py'>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import crawler104, config.search_params, async_example, async_job, jobs104\n",
    "import importlib\n",
    "importlib.reload(crawler104)\n",
    "importlib.reload(async_example)\n",
    "importlib.reload(async_job)\n",
    "importlib.reload(jobs104)\n",
    "importlib.reload(config.search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb4d88-569f-479a-8744-83b657f2fd1d",
   "metadata": {},
   "source": [
    "# 異步 網頁抓取 Web scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "aa68fd26-c5ae-4ad4-8da4-6b35abb7aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_jobs = filter_jobs_104[200:222]\n",
    "# filter_jobs = filter_jobs_104\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4d42f-4629-45b2-aabf-0de6a3eea82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs:22\n",
      "Batch Start\n",
      "永慶房產集團_永慶房屋仲介股份有限公司 | 數據分析師. | 2024/01/15\n",
      "松崗數位文創股份有限公司 | 數據分析產品規劃師 | 2024/01/15\n",
      "財團法人中衛發展中心 | 【數據專案企劃人員】-聯合招募 | 2024/01/15\n",
      "行動貝果有限公司 | Data Engineer 資料工程師 [8ndpoint] | 2024/01/15\n",
      "同協數位科技有限公司 | 前端工程師React.js | 2024/01/15\n",
      "德律科技股份有限公司 | R2: 3D AOI AI 研發工程師-222 | 2024/01/15\n",
      "龍騰文化事業股份有限公司 | 數據分析工程師【可農曆年後上班】 | 2024/01/15\n",
      "和碩集團_和碩聯合科技股份有限公司 | Android 韌體工程師_12550 | 2024/01/15\n",
      "賽威科技股份有限公司 | MBD軟體開發工程師 | 2024/01/15\n",
      "米波科技股份有限公司 | [遠端工作/WFH] App資深開發工程師 | 2024/01/15\n"
     ]
    }
   ],
   "source": [
    "# from crawler104 import Crawler104\n",
    "# jobs_link = crawler104.get_jobs_link(filter_jobs)\n",
    "# 網頁抓取 \n",
    "results = await async_job.scraper(filter_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9d32139d-3853-4fbf-87c1-56c9e6538646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://www.104.com.tw/job/81xz7',\n",
       "  '【工作內容】\\n網站開發人員\\n精通Python語言，並熟悉Django開發框架\\n熟悉異步編程（asynchronous programming）和多線程（multithreading）\\n熟悉Redis等緩存系統的使用和優化\\n設計和實現分散式系統的能力，包括負載均衡、故障轉移和數據恢復。\\n微服務架構相關經驗\\n對主要的雲平台部署 計算（AWS、Azure等）\\n或機器學習模型的整合和部署經驗\\n或\\n具備全端開發能力，jquery或前端任一框架的使用\\n- 職務類別：軟體工程師,Internet程式設計師,其他資訊專業人員\\n- 工作待遇：月薪70,000~120,000元\\n- 工作性質：全職\\n- 上班地點：台北市內湖區\\n- 管理責任：不需負擔管理責任\\n- 出差外派：無需出差外派\\n- 上班時段：日班\\n- 休假制度：週休二日\\n- 可上班日：不限\\n- 需求人數：1~2人\\n【相關條件】\\n- 工作經歷：不拘\\n- 學歷要求：大學以上\\n- 公司產業：藝文服務業\\n- 科系要求：數學及電算機科學學科類,物理學相關,工程學科類\\n- 語文條件：不拘\\n- 擅長工具：AJAX,Python,PostgreSQL,JavaScript,jQuery,Django\\n- 工作技能：伺服器網站管理維護,軟體程式設計,微電腦軟體設計,網路程式設計,Machine Learning,資料庫系統管理維護,資料庫程式設計\\n【其他條件】\\n【公司福利】\\n- 福利制度：1.勞健保\\n2.績效獎金\\n3.季度分紅\\n4.員工急難補助\\n更多工作資訊請參考：https://www.104.com.tw/job/81xz7'),\n",
       " ('https://www.104.com.tw/job/7ddc7',\n",
       "  '【工作內容】\\n主要職責：\\n* 網路架構規劃與管理。\\n* 系統備份、資訊安全規劃與管理。\\n* 雲端系統架構、虛擬化架構規劃與管理，提升服務可用性與擴展性。\\n* 監控與警示系統整合建置、Log 處理與分析。\\n* CI/CD自動化流程環境建置與管理。\\n* 協同自動化測試程式開發。\\n* 協同開發團隊調校系統最佳化，適時提出架構優化之建議。\\n專業條件 (必要條件)：\\n＊ 熟悉AWS基礎服務，ex: EC2、ECS、S3、CloudFront、RDS、LoadBalancer ...etc。\\n＊ 熟悉 Linux，擅長 Shell script 或其它 Scripting language。\\n* 熟悉網路知識，ex: TCP/IP、NAT、DNS、Firewall、Routing ...etc。\\n* 熟悉 Nginx觀念與操作。\\n加分項目：\\n* Docker、Kubernetes、Ansible等使用經驗。\\n* 關聯式資料庫管理或使用經驗(Oracle, SQL Server, MySQL, PostgreSQL等)。\\n* OLAP(Redshift、ClickHouse等)分析型資料庫使用經驗。\\n* MongoDB、Redis、ELK等使用經驗。\\n* GCP或Azure等其它雲端服務使用經驗。\\n* Go lang程式開發經驗。\\n* Web應用程式開發或架構經驗。\\n* 使用程式語言或工具開發自動化測試相關經驗，ex: Python、Selenium、Postman、Jmeter ...etc。\\n新創公司積極尋找人才，公司重視人才培訓，有良好的升遷空間。\\n環境舒適，公司福利優於勞基法。\\n薪資待遇：優於市場行情。\\n- 職務類別：網路管理工程師,系統工程師,網路安全分析師\\n- 工作待遇：月薪70,000元以上\\n- 工作性質：全職\\n- 上班地點：台北市南港區\\n- 管理責任：不需負擔管理責任\\n- 出差外派：無需出差外派\\n- 上班時段：日班\\n- 休假制度：週休二日\\n- 可上班日：一個月內\\n- 需求人數：1~3人\\n【相關條件】\\n- 工作經歷：2年以上\\n- 學歷要求：專科以上\\n- 公司產業：電腦軟體服務業\\n- 科系要求：不拘\\n- 語文條件：不拘\\n- 擅長工具：Linux,AWS,DNS,Firewall,Hubs/ Routers,TCP/IP\\n- 工作技能：伺服器網站管理維護,資料備份與復原,資訊設備環境設定,系統架構規劃,網路安全架構分析與設計\\n【其他條件】\\n【公司福利】\\n- 福利制度：※\\u3000保險類 :\\n1. 勞工保險\\n2. 全民健康保險\\n3. 勞工退休金(公司依法提撥6%)\\n4. 員工團體保險\\n※\\u3000請/休假制度 :\\n1. 特休假 :\\nA. 採預給特休制，到職即可安排特休。\\nB. 到職第一年為14天特休。\\nC. 到職第二年起，每年依勞基法法定特休年假規定，再多５天有薪個人假。\\n2. 全年有薪病假8天\\n3. 家庭照顧假\\n4. 產檢假\\n5. 男性同仁陪產假7天\\n6. 女性同仁生理假\\n7. 生日假\\n8. 彈性上下班時間\\n※\\u3000制度類 :\\n1. 試用期滿，績效考核及格者，調薪3%-10%\\n2. 任職滿一年，績效考核及格者，保障每年基本調薪3%-10%\\n3. 完整的教育訓練\\n4. 人性化的管理與愉悅的工作環境\\n5. 順暢的升遷機會\\n6. 公司內部技術分享與交流\\n※\\u3000福利 :\\n1. 結婚、生育、住院及喪葬補助金\\n2. 年度健康檢查\\n3. 不定期公司聚餐\\n4. 年終尾牙\\n5. 生日下午茶\\n6. 員工旅遊\\n7. 外部進修或教育訓練課程補助\\n8. 無限供應零食、咖啡、飲料\\n※\\u3000獎金 :\\n1. 年節禮金 : 端午、中秋禮金\\n2. 年終獎金\\n3. 全勤獎金\\n4. 生日禮金\\n5. 久任獎金\\n6. 績效獎金 (依各個部門規定)\\n7. 高額介紹獎金\\n8. 營運紅利獎金 (視公司營運狀況與個人績效而定)\\n年終獎金,三節獎金/禮品,生日假,不扣薪病假,結婚禮金,員工進修補助,旅遊補助,國內旅遊,國外旅遊,部門聚餐,慶生會,下午茶,電影觀賞,優於勞基法特休,員工團體保險,可遠端/在家上班\\n更多工作資訊請參考：https://www.104.com.tw/job/7ddc7')]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[0])\n",
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e228ed81-8954-4820-9bff-5a976d41773b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'async_example' from 'C:\\\\Users\\\\Rekam\\\\Documents\\\\Jobscan\\\\async_example.py'>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(async_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "473a9322-8ec6-491c-849d-253f2087a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花費 1.42 秒\n",
      "糖蛙線上娛樂股份有限公司 | JAVA後端工程師 | 2024/01/09\n",
      "花費 0.48 秒\n",
      "思銳生醫科技股份有限公司 | 軟體工程師 | 2024/01/11\n",
      "花費 0.52 秒\n",
      "星芽社會企業股份有限公司 | 後端工程師（Back-End Developer） | 2024/01/15\n",
      "花費 0.26 秒\n",
      "No script tag found.\n",
      "Error processing link https://www.104.com.tw/job/84qkq?jobsource=n104bank2: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "花費 0.88 秒\n",
      "天矽科技股份有限公司 | 網站後端工程師(新北市) | 2024/01/15\n",
      "花費 0.25 秒\n",
      "No script tag found.\n",
      "Error processing link https://www.104.com.tw/job/7iedv?jobsource=n104bank2: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "花費 0.34 秒\n",
      "長佳智能股份有限公司 | RD 後端工程師 | 2024/01/15\n",
      "花費 0.76 秒\n",
      "資拓宏宇國際股份有限公司 | Java後端工程師-政府領域(PS250) | 2024/01/15\n",
      "花費 0.68 秒\n",
      "數位無限軟體股份有限公司 | 後端工程師【台北】 | 2024/01/15\n",
      "花費 0.38 秒\n",
      "數位無限軟體股份有限公司 | 後端工程師【遠端】 | 2024/01/15\n",
      "花費 0.35 秒\n",
      "長佳智能股份有限公司 | 維運兼後端工程師 | 2024/01/15\n",
      "花費 0.38 秒\n",
      "寬橋有限公司 | 產品後端工程師 | 2024/01/15\n",
      "花費 0.63 秒\n",
      "升活商務顧問有限公司 | 後端工程師(Backend Engineer)－後端 (System Developer - Backend) #系統發展 | 2024/01/15\n",
      "花費 0.37 秒\n",
      "英仕國際有限公司 | 後端工程師 | 2024/01/15\n",
      "花費 0.6 秒\n",
      "立炘數位行銷有限公司 | 後端工程師 | 2024/01/15\n",
      "花費 0.37 秒\n",
      "伊普資訊股份有限公司 | Java後端工程師-金融 | 2024/01/15\n",
      "花費 1.01 秒\n",
      "眾匯智能健康股份有限公司 | 後端軟體工程師 | 2024/01/15\n",
      "花費 0.41 秒\n",
      "英仕國際有限公司 | 遊戲後端工程師 | 2024/01/15\n",
      "花費 0.68 秒\n",
      "博歐科技有限公司 | Senior Software Engineer, Backend 資深後端工程師 | 2024/01/15\n",
      "花費 0.38 秒\n",
      "ReMo瑞摩智能_肆時資訊設計有限公司 | 【ReMo】台中 - PHP / Laravel / CodeIgniter 後端工程師 | 2024/01/15\n"
     ]
    }
   ],
   "source": [
    "# # filter_jobs = filter_jobs_104[5:8]\n",
    "filter_jobs = filter_jobs_104[0:20]\n",
    "filter_jobs\n",
    "updated_list = [f\"https:{item['href']}\" for item in filter_jobs]\n",
    "updated_list\n",
    "# # html_content = await jobs104.get_info(filter_job)\n",
    "# jobs = await async_job.scraper(filter_jobs)\n",
    "\n",
    "results = await async_example.main(updated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d63903-6c73-4691-95b6-0597b10aba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f834e817-6adb-4f26-8063-fe9c10dd5354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "簡單智慧科技股份有限公司 | AI軟體開發工程師 | 2024/01/12 | 【工作內容】\n",
      "工作內容著重在開發以下深度學習,機器學習跟影像應用的軟體與服務,所需要的能力如下:\n",
      "1.具備深度學習與機器學習相關知識\n",
      "2.具備電腦視覺相關知識\n",
      "3.熟悉C/C+城市開發或有相關開發經驗(與4.擇一即可)\n",
      "4.熟悉網路服務開發或有相關經驗;主要著重於網站後台與資料庫應用(與3.擇一即可)\n",
      "5.熟悉Git\n",
      "6.熟悉Docker(Optional)\n",
      "7.熟悉Linux開發環境(Optional)\n",
      "8.對深度學習跟演算法有興趣,能閱讀理解論文內容\n",
      "9.對於理解問題的本質與解決問題有熱情\n",
      "以上1到4點為基本要求,5到9點為非必需但對評估工作內容是否適任有加分的作用;另外根據能力的高低與專精會提供不同的職位薪資\n",
      "- 職務類別：AI工程師,軟體工程師\n",
      "- 工作待遇：月薪50,000~80,000元\n",
      "- 工作性質：全職\n",
      "- 上班地點：台北市中山區\n",
      "- 管理責任：不需負擔管理責任\n",
      "- 出差外派：無需出差外派\n",
      "- 上班時段：日班\n",
      "- 休假制度：週休二日\n",
      "- 可上班日：不限\n",
      "- 需求人數：1人\n",
      "【相關條件】\n",
      "- 工作經歷：不拘\n",
      "- 學歷要求：大學、碩士\n",
      "- 公司產業：其他電信及通訊相關業\n",
      "- 科系要求：資訊工程相關,其他數學及電算機科學相關\n",
      "- 語文條件：不拘\n",
      "- 擅長工具：不拘\n",
      "- 工作技能：不拘\n",
      "【其他條件】\n",
      "- 其他：專業技能/執照 1.C/C++   2.Python   3.網路服務開發相關程式語言\n",
      "【公司福利】\n",
      "- 福利制度：1、周休二日，按照行政機關辦公日曆休假。\n",
      "2、上班時間：彈性上下班8:30-9:30；18:00-19:00 (午休12:00-13:30)。\n",
      "3、三節禮品、員工聚餐。\n",
      "4、依公司營運狀況及個人績效表現，發放年終獎金。\n",
      "5、其餘未述事宜比照勞基法。\n",
      "咖啡吧,慶生會,下午茶\n",
      "更多工作資訊請參考：https://www.104.com.tw/job/85wgm\n"
     ]
    }
   ],
   "source": [
    "soup99 = BeautifulSoup(jobs[0], 'html.parser')\n",
    "text_content = get_title(soup99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f11cc14d-d26f-496a-a5d1-a9ec20fb1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "簡單智慧科技股份有限公司 | AI軟體開發工程師 | 2024/01/12 | 【工作內容】\n",
      "工作內容著重在開發以下深度學習,機器學習跟影像應用的軟體與服務,所需要的能力如下:\n",
      "1.具備深度學習與機器學習相關知識\n",
      "2.具備電腦視覺相關知識\n",
      "3.熟悉C/C+城市開發或有相關開發經驗(與4.擇一即可)\n",
      "4.熟悉網路服務開發或有相關經驗;主要著重於網站後台與資料庫應用(與3.擇一即可)\n",
      "5.熟悉Git\n",
      "6.熟悉Docker(Optional)\n",
      "7.熟悉Linux開發環境(Optional)\n",
      "8.對深度學習跟演算法有興趣,能閱讀理解論文內容\n",
      "9.對於理解問題的本質與解決問題有熱情\n",
      "以上1到4點為基本要求,5到9點為非必需但對評估工作內容是否適任有加分的作用;另外根據能力的高低與專精會提供不同的職位薪資\n",
      "- 職務類別：AI工程師,軟體工程師\n",
      "- 工作待遇：月薪50,000~80,000元\n",
      "- 工作性質：全職\n",
      "- 上班地點：台北市中山區\n",
      "- 管理責任：不需負擔管理責任\n",
      "- 出差外派：無需出差外派\n",
      "- 上班時段：日班\n",
      "- 休假制度：週休二日\n",
      "- 可上班日：不限\n",
      "- 需求人數：1人\n",
      "【相關條件】\n",
      "- 工作經歷：不拘\n",
      "- 學歷要求：大學、碩士\n",
      "- 公司產業：其他電信及通訊相關業\n",
      "- 科系要求：資訊工程相關,其他數學及電算機科學相關\n",
      "- 語文條件：不拘\n",
      "- 擅長工具：不拘\n",
      "- 工作技能：不拘\n",
      "【其他條件】\n",
      "- 其他：專業技能/執照 1.C/C++   2.Python   3.網路服務開發相關程式語言\n",
      "【公司福利】\n",
      "- 福利制度：1、周休二日，按照行政機關辦公日曆休假。\n",
      "2、上班時間：彈性上下班8:30-9:30；18:00-19:00 (午休12:00-13:30)。\n",
      "3、三節禮品、員工聚餐。\n",
      "4、依公司營運狀況及個人績效表現，發放年終獎金。\n",
      "5、其餘未述事宜比照勞基法。\n",
      "咖啡吧,慶生會,下午茶\n",
      "更多工作資訊請參考：https://www.104.com.tw/job/85wgm\n"
     ]
    }
   ],
   "source": [
    "# 使用 BeautifulSoup 解析 HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup99 = BeautifulSoup(html_content, 'html.parser')\n",
    "text_content = get_title(soup99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f5a0a-417f-4c8a-be7d-3bf1083e2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# 在 Jupyter 中處理異步事件循環的嵌套\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "async def scrape_job_data(item):\n",
    "    start_time = time.time()\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https:{item['href']}\"\n",
    "    url = re.sub(r'\\?.*$', '', url)\n",
    "\n",
    "    # 启动 Chrome 浏览器\n",
    "    option = Options()\n",
    "    option.page_load_strategy = 'none'\n",
    "    option.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "    \n",
    "    browser = webdriver.Chrome(options=option)\n",
    "    \n",
    "    try:\n",
    "        # 直接使用 await，因為 browser.get 和 browser.wait 本身就是異步的\n",
    "        await browser.get(url)\n",
    "        await browser.wait(EC.presence_of_element_located((By.ID, 'app')), timeout=10)\n",
    "        # 获取加载后的页面内容\n",
    "        html_content = browser.page_source\n",
    "    finally:\n",
    "        # 关闭浏览器\n",
    "        browser.quit()\n",
    "\n",
    "    # 使用 BeautifulSoup 解析 HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # 測試列印\n",
    "    get_title(soup)\n",
    "    # 打印输出漂亮格式化的 HTML\n",
    "    # print(soup.prettify())\n",
    "    print(f\"花費 {round((time.time() - start_time),2)} 秒\")\n",
    "    return soup\n",
    "\n",
    "# Assuming you have a list of jobs to scrape\n",
    "filter_jobs = filter_jobs_104[5:8]\n",
    "\n",
    "# 使用異步方式運行\n",
    "async def main():\n",
    "    # 設定並發數量\n",
    "    max_concurrent = 5\n",
    "    \n",
    "    # 使用異步方式運行\n",
    "    tasks = [scrape_job_data(item) for item in filter_jobs]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# result  = await main()  # 直接调用 main() 而不使用 asyncio.gather()\n",
    "\n",
    "# 創建事件循環\n",
    "loop = asyncio.get_event_loop()\n",
    "# 運行主函數\n",
    "loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3da5c9c-3f78-42be-a7eb-31c3c5d8842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (Jr/Sr.) 後端工程師 Backend Engineer (PHP / Laravel)\n",
      "1 https://www.104.com.tw/job/6qlwp\n"
     ]
    }
   ],
   "source": [
    "import async_job\n",
    "import jobs104\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# result = await async_job.scraper(filter_jobs_104)\n",
    "# filter_jobs_104\n",
    "item = filter_jobs_104[20]\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'\n",
    "    }\n",
    "# item = \"https://www.104.com.tw/job/822rt\"\n",
    "# await jobs104.get_info(filter_jobs_104[0])\n",
    "from aiohttp import ClientSession, TCPConnector\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# async def fetch(session, url):\n",
    "#         async with session.get(url, headers = {'User-Agent':'GoogleBot'}) as response:\n",
    "#             return await response.text()\n",
    "# 启动 Chrome 浏览器\n",
    "option = Options()\n",
    "option.page_load_strategy = 'none'\n",
    "option.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "# option.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "# option.add_argument('--headless') # 瀏覽器不提供頁面觀看，linux下如果系統是純文字介面不加這條會啓動失敗\n",
    "# option.add_argument('--disable-dev-shm-usage') # 使用共享內存RAM\n",
    "# option.add_argument('--disable-gpu') # 規避部分chrome gpu bug\n",
    "# option.add_experimental_option(\"prefs\", prefs)\n",
    "# option.add_argument('blink-settings=imagesEnabled=false') #不加載圖片提高效率\n",
    "\n",
    "async def fetch(session,url):\n",
    "    async with session.get(url, headers = {'User-Agent':'GoogleBot'}) as response:\n",
    "        return await response.text()\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    title = item['title']\n",
    "    print(1, title)\n",
    "    url = f\"https:{item['href']}\"\n",
    "    url = re.sub(r'\\?.*$', '', url)\n",
    "\n",
    "    print(1, url)\n",
    "\n",
    "    browser = webdriver.Chrome(options=option)\n",
    "    # 打开网页\n",
    "    browser.get(url)\n",
    "    # 显式等待，等待页面加载完成\n",
    "    WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'app')))\n",
    "    # 获取加载后的页面内容\n",
    "    html_content = browser.page_source\n",
    "    # 关闭浏览器\n",
    "    browser.quit()\n",
    "    # 使用 BeautifulSoup 解析 HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "import asyncio\n",
    "# soup = await asyncio.gather(main())\n",
    "soup = await main()  # 直接调用 main() 而不使用 asyncio.gather()\n",
    "# print(3, soup)\n",
    "# print(1,title)\n",
    "# print(2,job_link)\n",
    "# print(3,response)\n",
    "# print(4,soup)\n",
    "\n",
    "# data-v-a9f35207 class\n",
    "# data-v-a9f35207"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9702f6c8-4cd6-4a42-a291-8afbb1268c0b",
   "metadata": {},
   "source": [
    "# 多執行緒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a6ece9e-8cfb-47e5-85e8-aff1930a4576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花費 6.36 秒\n",
      "花費 6.44 秒\n",
      "花費 7.12 秒\n",
      "花費 7.13 秒\n",
      "花費 7.75 秒\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Assuming filter_jobs_104 and np are defined elsewhere in your code\n",
    "\n",
    "def scrape_job_data(item):\n",
    "    start_time = time.time()\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https:{item['href']}\"\n",
    "    url = re.sub(r'\\?.*$', '', url)\n",
    "\n",
    "    # 启动 Chrome 浏览器\n",
    "    option = Options()\n",
    "    option.page_load_strategy = 'none'\n",
    "    option.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "    \n",
    "    with webdriver.Chrome(options=option) as browser:\n",
    "        # 打开网页\n",
    "        browser.get(url)\n",
    "        # 显式等待，等待页面加载完成\n",
    "        WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'app')))\n",
    "        # 获取加载后的页面内容\n",
    "        html_content = browser.page_source\n",
    "    \n",
    "    # 使用 BeautifulSoup 解析 HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # 打印输出漂亮格式化的 HTML\n",
    "    # print(soup.prettify())\n",
    "    print(f\"花費 {np.round((time.time() - start_time),2)} 秒\")\n",
    "\n",
    "# Assuming you have a list of jobs to scrape\n",
    "filter_jobs= filter_jobs_104[0:5]\n",
    "\n",
    "# Set the maximum number of concurrent threads\n",
    "max_threads = 5\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_threads) as executor:\n",
    "    # Submit each job for scraping concurrently\n",
    "    futures = [executor.submit(scrape_job_data, item) for item in filter_jobs]\n",
    "\n",
    "    # Wait for all futures to complete\n",
    "    for future in futures:\n",
    "        future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8ed5a2ee-ba99-44b9-8747-7d77af4c1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花費 4.17 秒\n",
      "花費 4.42 秒\n",
      "花費 4.07 秒\n",
      "花費 4.09 秒\n",
      "花費 4.12 秒\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# 在 Jupyter 中處理異步事件循環的嵌套\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_job_data(item):\n",
    "    start_time = time.time()\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https:{item['href']}\"\n",
    "    url = re.sub(r'\\?.*$', '', url)\n",
    "\n",
    "    # 启动 Chrome 浏览器\n",
    "    option = Options()\n",
    "    option.page_load_strategy = 'none'\n",
    "    option.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "    \n",
    "    browser = webdriver.Chrome(options=option)\n",
    "    \n",
    "    try:\n",
    "        # 打开网页\n",
    "        browser.get(url)\n",
    "        # 显式等待，等待页面加载完成\n",
    "        WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'app')))\n",
    "        # 获取加载后的页面内容\n",
    "        html_content = browser.page_source\n",
    "    finally:\n",
    "        # 关闭浏览器\n",
    "        browser.quit()\n",
    "\n",
    "    # 使用 BeautifulSoup 解析 HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # 測試列印\n",
    "    get_title(soup)\n",
    "    # 打印输出漂亮格式化的 HTML\n",
    "    # print(soup.prettify())\n",
    "    print(f\"花費 {round((time.time() - start_time),2)} 秒\")\n",
    "\n",
    "# Assuming you have a list of jobs to scrape\n",
    "filter_jobs = filter_jobs_104[0:5]\n",
    "\n",
    "# 使用異步方式運行\n",
    "async def main():\n",
    "    # 設定並發數量\n",
    "    max_concurrent = 5\n",
    "    \n",
    "    # 使用異步方式運行\n",
    "    tasks = [scrape_job_data(item) for item in filter_jobs]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# 創建事件循環\n",
    "loop = asyncio.get_event_loop()\n",
    "# 運行主函數\n",
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe4c9d-d439-46d2-a820-e2915be15c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "start_time = time.time()\n",
    "\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "\n",
    "item = filter_jobs_104[120]\n",
    "url = f\"https:{item['href']}\"\n",
    "url = re.sub(r'\\?.*$', '', url)\n",
    "\n",
    "# 启动 Chrome 浏览器\n",
    "option = Options()\n",
    "option.page_load_strategy = 'none'\n",
    "option.add_argument(f\"user-agent={headers['User-Agent']}\")\n",
    "# option.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "# option.add_argument('--headless') # 瀏覽器不提供頁面觀看，linux下如果系統是純文字介面不加這條會啓動失敗\n",
    "# option.add_argument('--disable-dev-shm-usage') # 使用共享內存RAM\n",
    "# option.add_argument('--disable-gpu') # 規避部分chrome gpu bug\n",
    "# option.add_experimental_option(\"prefs\", prefs)\n",
    "# option.add_argument('blink-settings=imagesEnabled=false') #不加載圖片提高效率\n",
    "\n",
    "browser = webdriver.Chrome(options=option)\n",
    "# 打开网页\n",
    "browser.get(url)\n",
    "# 显式等待，等待页面加载完成\n",
    "WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'app')))\n",
    "# 获取加载后的页面内容\n",
    "html_content = browser.page_source\n",
    "# 关闭浏览器\n",
    "browser.quit()\n",
    "# 使用 BeautifulSoup 解析 HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "# soup = BeautifulSoup(response, 'lxml')\n",
    "# 打印输出漂亮格式化的 HTML\n",
    "# print(soup.prettify())\n",
    "print(f\"花費 {np.round((time.time() - start_time),2)} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c79fa1cb-8887-4a03-a089-65be15286b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import html\n",
    "\n",
    "\n",
    "def get_title(soup):\n",
    "    if soup is None:\n",
    "        print(\"No data available.\")\n",
    "        return None\n",
    "\n",
    "    # 找到包含JavaScript代码的script标签\n",
    "    script_tag = soup.find('script', type='application/ld+json')\n",
    "\n",
    "    if script_tag is None:\n",
    "        print(\"No script tag found.\")\n",
    "        return None\n",
    "\n",
    "    json_string = script_tag.text\n",
    "\n",
    "    # 将JSON字符串转换为Python的list\n",
    "    data_list = json.loads(json_string)\n",
    "\n",
    "    # 打印转换后的list\n",
    "    company = data_list[0]['itemListElement'][1]['name']\n",
    "    print(company, end=\" | \")\n",
    "\n",
    "    position = data_list[0]['itemListElement'][2]['name']\n",
    "    print(position, end=\" | \")\n",
    "\n",
    "    update = data_list[2]['datePosted']\n",
    "    print(update, end=\" | \")\n",
    "\n",
    "    description = data_list[2]['description']\n",
    "    description = html.unescape(description)\n",
    "\n",
    "    # 使用 BeautifulSoup 解析 HTML\n",
    "    soup_descript = BeautifulSoup(description, 'html.parser')\n",
    "\n",
    "    # 将 <br> 转换成换行符号\n",
    "    for br_tag in soup_descript.find_all('br'):\n",
    "        br_tag.replace_with('\\n')\n",
    "\n",
    "    # 提取纯文本内容\n",
    "    text_content = soup_descript.get_text(separator='\\n', strip=True)\n",
    "    print(text_content)\n",
    "    return text_content\n",
    "    \n",
    "    \n",
    "    # 打印排版後的文本\n",
    "    # print(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a3cd318-b2c6-44ef-b6a4-e2ea51f595a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"zh-Hant-TW\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"width=device-width,initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <meta content=\"telephone=no\" name=\"format-detection\"/>\n",
      "  <link href=\"android-app://com.m104/m104/job?j=4b514a753f7350274b49422459353d6643f4149723b3d456e434941242424246974d41497513j98&amp;jobNo=822rt\" rel=\"alternate\"/>\n",
      "  <link href=\"ios-app://437817158/m104/job?j=4b514a753f7350274b49422459353d6643f4149723b3d456e434941242424246974d41497513j98&amp;jobNo=822rt\" rel=\"alternate\"/>\n",
      "  <meta content=\"on\" http-equiv=\"x-dns-prefetch-control\"/>\n",
      "  <link href=\"https://static.104.com.tw\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"//cdn.104.com.tw\" rel=\"dns-prefetch\"/>\n",
      "  <!-- Google Tag Manager -->\n",
      "  <script async=\"\" src=\"https://connect.facebook.net/en_US/fbevents.js\">\n",
      "  </script>\n",
      "  <script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=AW-820745912&amp;l=dataLayer&amp;cx=c\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=AW-992115811&amp;l=dataLayer&amp;cx=c\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-WYQPBGBV8Z&amp;l=dataLayer&amp;cx=c\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-FJWMQR9J2K&amp;l=dataLayer&amp;cx=c\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=G-W9X1GB1SVR&amp;l=dataLayer&amp;cx=c\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script async=\"\" src=\"https://www.googletagmanager.com/gtm.js?id=GTM-P379T3\">\n",
      "  </script>\n",
      "  <script>\n",
      "   (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n",
      "            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\n",
      "            'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n",
      "        })(window,document,'script','dataLayer','GTM-P379T3');\n",
      "  </script>\n",
      "  <!-- End Google Tag Manager -->\n",
      "  <script src=\"https://static.104.com.tw/104i/js/api/log/e104.log.latest.js?v=20190424\">\n",
      "  </script>\n",
      "  <script src=\"https://static.104.com.tw/104main/libs/mit/jquery/3.3.1/jquery.min.js\">\n",
      "  </script>\n",
      "  <script src=\"https://static.104.com.tw/104main/libs/mit/jquery-migrate/3.0.1/jquery-migrate.min.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script src=\"https://static.104.com.tw/104main/libs/mit/jquery-migrate/1.4.1/jquery-migrate.min.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script src=\"https://static.104.com.tw/bubble/js/mainjs.js?by=jbbar&amp;v=20200323\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <link href=\"//static.104.com.tw/bubble/css/alert.css?v=20140701\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <script src=\"//www.104.com.tw/jobbank/globalbar/globaljson_website.js?v=20140701\">\n",
      "  </script>\n",
      "  <script language=\"javascript\" src=\"//bubble.104.com.tw/js/showbarright\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script src=\"//www.104.com.tw/jobbank/globalbar/globaljson_more.js?v=20140701\">\n",
      "  </script>\n",
      "  <script src=\"//www.104.com.tw/jobbank/globalbar/globaljson_notice.js?v=20140701\">\n",
      "  </script>\n",
      "  <script src=\"//static.104.com.tw/104main/js/jquery/jquery.bgiframe.js?v=20140701\">\n",
      "  </script>\n",
      "  <script src=\"//static.104.com.tw/bubble/js/jquery.104bar.js?v=20140701\">\n",
      "  </script>\n",
      "  <link href=\"//cdn.104.com.tw/designer-icons/1.1.24/style.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/FooterFixedConnect.9de34067.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/BreadCrumb.c523fd9b.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/AnalysisBars.b06041f0.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/ApplyButton.e9ad2c53.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/GoTopRounded.8468dbe6.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/Tag.0e2ddfe0.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/examLogin.a62f9c79.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/job.f9de9149.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/My104ContainerSidebar.80627581.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/modernizr.e3b0c442.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/filterShareBarProps.e04fd97a.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/PopupMedium.40afa6f1.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/Apply.606eb733.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/JobInfo.c7cb6d61.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/JobSummery.f32c5b8e.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/JobCard.e659c346.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/SidebarDialog.abd89917.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/rollbar.3d3da459.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/lite.b25a530b.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/FooterFullsize.675f978d.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/Loading.233da9ec.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/LogoHeaderPc.c2d5496c.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/LogoHeaderRwd.f7dd106f.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/ErrorContainer.c5e49912.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/ErrorDialog.88d588db.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/CopiedPopup.0c928cb6.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/SharePopupMobile.ab7d3ca4.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/Navigation.a2b3e751.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/AppBanner.dac740fe.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/FooterCMW.966d6a76.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/CLoadingPlugin.0e85dfb8.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//cdn.104.com.tw/cindex/assets/css/index.5aed384d.css\" rel=\"stylesheet\"/>\n",
      "  <script defer=\"\" src=\"https://msc.adsmart.104.com.tw/js/adsmart-ui.js\">\n",
      "  </script>\n",
      "  <script defer=\"\" src=\"//cdn.104.com.tw/cindex/assets/js/cindex-job.dc14265a.js\" type=\"module\">\n",
      "  </script>\n",
      "  <script type=\"module\">\n",
      "   try{import.meta.url;import(\"_\").catch(()=>1);}catch(e){}window.__vite_is_modern_browser=true;\n",
      "  </script>\n",
      "  <script type=\"module\">\n",
      "   !function(){if(window.__vite_is_modern_browser)return;console.warn(\"vite: loading legacy build because dynamic import or import.meta.url is unsupported, syntax error above should be ignored\");var e=document.getElementById(\"vite-legacy-polyfill\"),n=document.createElement(\"script\");n.src=e.src,n.onload=function(){System.import(document.getElementById('vite-legacy-entry').getAttribute('data-src'))},document.body.appendChild(n)}();\n",
      "  </script>\n",
      " </head>\n",
      " <body>\n",
      "  <!-- Google Tag Manager (noscript) -->\n",
      "  <noscript>\n",
      "   <iframe height=\"0\" src=\"https://www.googletagmanager.com/ns.html?id=GTM-P379T3\" style=\"display:none;visibility:hidden\" width=\"0\">\n",
      "   </iframe>\n",
      "  </noscript>\n",
      "  <!-- End Google Tag Manager (noscript) -->\n",
      "  <div id=\"globalbar\">\n",
      "   <div id=\"bar_m104\">\n",
      "    <div id=\"global_bk\">\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      "  <noscript>\n",
      "   <strong>\n",
      "    We're sorry but vue-start doesn't work properly without JavaScript enabled. Please enable it to continue.\n",
      "   </strong>\n",
      "  </noscript>\n",
      "  <div id=\"app\">\n",
      "  </div>\n",
      "  <script defer=\"\" src=\"https://static.104.com.tw/104main/libs/mit/vue/2.6.12/vue.min.js\">\n",
      "  </script>\n",
      "  <script nomodule=\"\">\n",
      "   !function(){var e=document,t=e.createElement(\"script\");if(!(\"noModule\"in t)&&\"onbeforeload\"in t){var n=!1;e.addEventListener(\"beforeload\",(function(e){if(e.target===t)n=!0;else if(!e.target.hasAttribute(\"nomodule\")||!n)return;e.preventDefault()}),!0),t.type=\"module\",t.src=\".\",e.head.appendChild(t),t.remove()}}();\n",
      "  </script>\n",
      "  <script crossorigin=\"\" id=\"vite-legacy-polyfill\" nomodule=\"\" src=\"//cdn.104.com.tw/cindex/assets/js/polyfills-legacy.5a3c9cc5.js\">\n",
      "  </script>\n",
      "  <script crossorigin=\"\" data-src=\"//cdn.104.com.tw/cindex/assets/js/cindex-job-legacy.0e1a9634.js\" id=\"vite-legacy-entry\" nomodule=\"\">\n",
      "   System.import(document.getElementById('vite-legacy-entry').getAttribute('data-src'))\n",
      "  </script>\n",
      "  <script id=\"\" type=\"text/javascript\">\n",
      "   !function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version=\"2.0\",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,\"script\",\"https://connect.facebook.net/en_US/fbevents.js\");fbq(\"init\",\"1607446572852316\");fbq(\"track\",\"PageView\");\n",
      "  </script>\n",
      "  <noscript>\n",
      "   <img height=\"1\" src=\"https://www.facebook.com/tr?id=1607446572852316&amp;ev=PageView&amp;noscript=1\" style=\"display:none\" width=\"1\"/>\n",
      "  </noscript>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# soup\n",
    "# def update_date(self, soup) -> str:\n",
    "# update_date = soup.find(\"div\", class_=\"job-header__title\")\n",
    "soup.prettify()\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 使用 prettify() 方法格式化输出\n",
    "formatted_html = soup.prettify()\n",
    "\n",
    "# 打印输出\n",
    "print(formatted_html)\n",
    "# update_date = soup.find(\"ul\", class_=\"global_nav\")\n",
    "# print(update_date)\n",
    "        # return update_date.find('span').text.strip().replace('更新','') if update_date else '無'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489de1dd-bb38-4970-90db-35475c87c679",
   "metadata": {},
   "source": [
    "# 參考Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90e067a-dbad-4979-95f0-7b0a2e95e714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class eJob_search104():\n",
    "    \n",
    "    \n",
    "    def update_date(self, soup) -> str:\n",
    "        update_date = soup.find(\"div\", class_=\"job-header__title\")\n",
    "        return update_date.find('span').text.strip().replace('更新','') if update_date else '無'\n",
    "    \n",
    "    def company(self, soup) -> str:\n",
    "        name = soup.find(\"div\", class_=\"mt-3\")\n",
    "        return name.select_one('div > a').text.strip() if name else '無'\n",
    "    \n",
    "    def jd_info(self, soup) -> dict:\n",
    "        result = {}\n",
    "        JD = soup.find('div', class_='job-description-table row')\n",
    "        if JD:\n",
    "            jd_items = JD.find_all('div', recursive=False)\n",
    "            if jd_items:\n",
    "                    try:\n",
    "                        job_content = jd_items[0].find('p').text if jd_items[0].find('p').text else '無'\n",
    "                    except:\n",
    "                        job_content = '無'\n",
    "\n",
    "                    try:\n",
    "                        job_category = ', '.join(i.text for i in jd_items[1].find_all('u')) if jd_items[1].find_all('u') else '無'\n",
    "                    except:\n",
    "                        job_category = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        salary = jd_items[2].find_all('div', recursive=False)[-1].text.strip() if jd_items[2].find_all('div', recursive=False)[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        salary = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        job_type = jd_items[3].find_all('div')[-1].text.strip() if jd_items[3].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        job_type = '無'\n",
    "\n",
    "                    try:\n",
    "                        workingplace = jd_items[4].find_all('div')[-1].text.strip() if jd_items[4].find_all('div')[-1].text.strip() != '代企業徵才' else None\n",
    "                    except:\n",
    "                        workingplace = '無'\n",
    "\n",
    "                    try:\n",
    "                        management_responsibility = jd_items[6].find_all('div')[-1].text.strip() if jd_items[6].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        management_responsibility = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        business_trip = jd_items[7].find_all('div')[-1].text.strip() if jd_items[7].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        business_trip = '無'\n",
    "\n",
    "                    try:\n",
    "                        working_duration = jd_items[8].find_all('div')[-1].text.strip() if jd_items[8].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        working_duration = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        Holiday_System = jd_items[9].find_all('div')[-1].text.strip() if jd_items[9].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        Holiday_System = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        Working_date = jd_items[10].find_all('div')[-1].text.strip() if jd_items[10].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        Working_date = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        ppl_required = jd_items[11].find_all('div')[-1].text.strip() if jd_items[11].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        ppl_required = '無'\n",
    "\n",
    "\n",
    "                    result = {\n",
    "                                '工作內容': job_content,\n",
    "                                '職務類別': job_category,\n",
    "                                '工作待遇': salary,\n",
    "                                '工作性質': job_type,\n",
    "                                '上班地點': workingplace,\n",
    "                                '管理責任': management_responsibility,\n",
    "                                '出差外派': business_trip,\n",
    "                                '上班時段': working_duration,\n",
    "                                '休假制度': Holiday_System,\n",
    "                                '可上班日': Working_date,\n",
    "                                '需求人數': ppl_required\n",
    "                            }\n",
    "        return result\n",
    "    \n",
    "    def jr_info(self, soup) -> dict:\n",
    "        result = {}\n",
    "        JR = soup.find('div', class_= 'job-requirement-table row')\n",
    "        JRO = soup.find('div', class_= 'job-requirement col opened')\n",
    "        if JR:\n",
    "            jr_items = JR.find_all('div', recursive=False)\n",
    "            if jr_items:\n",
    "                try:\n",
    "                    work_exp = jr_items[0].find_all('div')[-1].text.strip() if jr_items[0].find_all('div')[-1].text.strip() else '無'\n",
    "                except:\n",
    "                    work_exp = '無'\n",
    "                \n",
    "                try:\n",
    "                    academic_require = jr_items[1].find_all('div')[-1].text.strip() if jr_items[1].find_all('div')[-1].text.strip() else '無'\n",
    "                except:\n",
    "                    academic_require = '無'\n",
    "                \n",
    "                try:\n",
    "                    department_require = jr_items[2].find_all('div')[-1].text.strip() if jr_items[2].find_all('div')[-1].text.strip() else '無'\n",
    "                except:\n",
    "                    department_require = '無'\n",
    "\n",
    "                try:\n",
    "                    language = jr_items[3].find('p').text.strip() if jr_items[3].find('p').text.strip() else '無'\n",
    "                except:\n",
    "                    language = '無'\n",
    "\n",
    "                try:\n",
    "                    tool = ', '.join(i.text for i in jr_items[4].find_all('u')) if jr_items[4].find_all('u') else '無'\n",
    "                except:\n",
    "                    tool = '無'\n",
    "                \n",
    "                try:\n",
    "                    working_ability = jr_items[5].find_all('div')[-1].text.strip() if jr_items[5].find_all('div')[-1].text.strip() else '無'\n",
    "                except:\n",
    "                    working_ability = '無'\n",
    "   \n",
    "        try:\n",
    "            others = JRO.find_all('div')[-1].text.strip() if JRO.find_all('div')[-1].text.strip() else '無'\n",
    "        except:\n",
    "            others = '無'\n",
    "\n",
    "        if JR and JRO:\n",
    "            result = {\n",
    "                '工作經歷' : work_exp,\n",
    "                '學歷要求' : academic_require,\n",
    "                '科系要求' : department_require,\n",
    "                '語文條件' : language,\n",
    "                '擅長工具' : tool,\n",
    "                '工作技能' : working_ability,\n",
    "                '其他要求' : others\n",
    "            }\n",
    "\n",
    "        # result = {\n",
    "        #     '工作經歷' : work_exp,\n",
    "        #     '學歷要求' : academic_require,\n",
    "        #     '科系要求' : department_require,\n",
    "        #     '語文條件' : language,\n",
    "        #     '擅長工具' : tool,\n",
    "        #     '工作技能' : working_ability,\n",
    "        #     '其他要求' : others\n",
    "        # }\n",
    "        return result\n",
    "    \n",
    "    async def fetch(self, session, url):\n",
    "        async with session.get(url, headers = {'User-Agent':'GoogleBot'}) as response:\n",
    "            return await response.text()\n",
    "\n",
    "    async def get_job_info(self, item):\n",
    "        try:\n",
    "            title = item['title']\n",
    "            Job_link = f\"https:{item['href']}\"\n",
    "            connector = TCPConnector(limit=10)\n",
    "            async with ClientSession(connector=connector) as session:\n",
    "                html = await self.fetch(session, Job_link)\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            Data = {\n",
    "                '更新日期': [self.update_date(soup)],\n",
    "                '職缺名稱': [title],\n",
    "                '公司名稱': [self.company(soup)],\n",
    "                '連結': [Job_link]\n",
    "            }\n",
    "            Data.update(self.jd_info(soup))\n",
    "            Data.update(self.jr_info(soup))\n",
    "            df = pd.DataFrame(Data, columns=['更新日期', '職缺名稱', '公司名稱', '工作內容', '職務類別', '工作待遇',\n",
    "                                            '工作性質', '上班地點', '管理責任', '出差外派', '上班時段', '休假制度',\n",
    "                                            '可上班日', '需求人數', '工作經歷', '學歷要求', '科系要求', '語文條件',\n",
    "                                            '擅長工具', '工作技能', '其他要求', '連結'])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(\"發生錯誤\", e)\n",
    "            return None\n",
    "\n",
    "    async def scrape(self, Job_list):\n",
    "        tasks = []\n",
    "        semaphore = asyncio.Semaphore(10)  # Limit concurrent requests to 10\n",
    "\n",
    "        for item in Job_list:\n",
    "            async with semaphore:\n",
    "                task = asyncio.ensure_future(self.get_job_info(item))\n",
    "                tasks.append(task)\n",
    "\n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "    def main(self, Job_list: list, DF):\n",
    "        print(len(Job_list))\n",
    "        batch_size = 30\n",
    "        num_batches = (len(Job_list) + batch_size - 1) // batch_size\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(Job_list))\n",
    "            Job_list_batch = Job_list[start_idx:end_idx]\n",
    "            loop = asyncio.get_event_loop()\n",
    "            results = loop.run_until_complete(self.scrape(Job_list_batch))\n",
    "            # loop.close()\n",
    "\n",
    "            for df in results:\n",
    "                if df is not None:\n",
    "                    DF = pd.concat([DF, df], ignore_index=True)\n",
    "\n",
    "        DF.to_csv(f\"./output/JBLIST_{self.current_date}.csv\", sep=',', index=False)\n",
    "        return DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f8917-6fb6-4b01-94ff-098a746f5938",
   "metadata": {},
   "source": [
    "# 測試範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e7b2d4-a954-4452-98fc-833e3e40e7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start asynchronous task 0\n",
      "Start asynchronous task 1\n",
      "Start asynchronous task 2\n",
      "Start asynchronous task 3\n",
      "Start asynchronous task 4\n",
      "End asynchronous task 0\n",
      "End asynchronous task 2\n",
      "End asynchronous task 4\n",
      "End asynchronous task 1\n",
      "End asynchronous task 3\n",
      "['Result of task 0', 'Result of task 1', 'Result of task 2', 'Result of task 3', 'Result of task 4']\n"
     ]
    }
   ],
   "source": [
    "# 如果你在一個 Python script 中執行這樣的程式碼，你可能需要使用 asyncio.run() 函數。\n",
    "# 在異步程式碼中，如果你使用 asyncio.gather 函數收集異步任務的結果，而這些任務沒有顯式返回值，\n",
    "# gather 函數將返回一個包含每個異步任務結果的列表，而這些結果通常是 None。\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def async_example(i):\n",
    "    print(f\"Start asynchronous task {i}\")\n",
    "    await asyncio.sleep(1)\n",
    "    print(f\"End asynchronous task {i}\")\n",
    "    return f\"Result of task {i}\"\n",
    "\n",
    "# 直接在 Jupyter cell 中執行\n",
    "tasks = [async_example(i) for i in range(5)]\n",
    "results = await asyncio.gather(*tasks)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf2de66-3b16-4ee2-ade2-b53364472101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1 - (1-30): 100%|██████████| 30/30 [00:01<00:00, 29.18task/s]\n",
      "Batch 2 - (31-60): 100%|██████████| 30/30 [00:01<00:00, 29.18task/s]\n",
      "Batch 3 - (61-90): 100%|██████████| 30/30 [00:01<00:00, 29.25task/s]\n",
      "Batch 4 - (91-120): 100%|██████████| 30/30 [00:01<00:00, 28.15task/s]\n",
      "Batch 5 - (121-150): 100%|██████████| 30/30 [00:01<00:00, 29.22task/s]\n",
      "Batch 6 - (151-180): 100%|██████████| 30/30 [00:01<00:00, 29.20task/s]\n",
      "Batch 7 - (181-210): 100%|██████████| 30/30 [00:01<00:00, 29.19task/s]\n",
      "Batch 8 - (211-240): 100%|██████████| 30/30 [00:01<00:00, 29.18task/s]\n",
      "Batch 9 - (241-270): 100%|██████████| 30/30 [00:01<00:00, 28.94task/s]\n",
      "Batch 10 - (271-300): 100%|██████████| 30/30 [00:01<00:00, 28.02task/s]\n",
      "Batch 11 - (301-330): 100%|██████████| 30/30 [00:01<00:00, 29.35task/s]\n",
      "Batch 12 - (331-338): 100%|██████████| 8/8 [00:01<00:00,  7.93task/s]\n"
     ]
    }
   ],
   "source": [
    "# 執行 async_example.py 範例\n",
    "from async_example import main\n",
    "result = await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1772d716-6079-440f-8a60-c246b1019682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, time, requests, random\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import grequests # 看起要在.py才能用\n",
    "from aiohttp import ClientSession, TCPConnector\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9e1302-0331-48a3-8b3d-f3e23e129d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eJob_search104():\n",
    "    current_date = datetime.now().date()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    search_url = 'https://www.104.com.tw/jobs/search/?'\n",
    "    def __init__(self, filter_params, key_word, page = 15):\n",
    "        self.filter_params = filter_params\n",
    "        self.key_word = key_word\n",
    "        self.page = page\n",
    "           \n",
    "    def search_job(self):\n",
    "        url = requests.get(self.search_url, self.filter_params, headers=self.headers).url\n",
    "        print(url)\n",
    "        option = Options()\n",
    "        option.add_argument(f\"user-agent={self.headers['User-Agent']}\")\n",
    "        option.add_experimental_option('excludeSwitches', ['enable-automation']) # 開發者模式。可以避開某些防爬機制，有開有保佑\n",
    "        # option.add_argument('--headless') # 無頭模式，開發完成之後再使用，可以完全背景執行，有機會變快\n",
    "        # option.add_argument(\"--disable-gpu\") # 禁用GPU加速，有些情況下需要設置這個參數\n",
    "        driver = webdriver.Chrome(options=option)\n",
    "        driver.get(url)\n",
    "\n",
    "        element = driver.find_element(By.XPATH,'//*[@id=\"js-job-header\"]/div[1]/label[1]/select/option[1]')\n",
    "        total_page = int(re.sub(r'\\D', '', element.text.split('/')[-1]))\n",
    "        print(f'Total_page = {total_page}')\n",
    "        # 滾頁面\n",
    "        scroll_times = self.page\n",
    "        for _ in range(scroll_times):\n",
    "            driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "            time.sleep(2)\n",
    "\n",
    "        # 自動加載結束後要自行點選載入(15以後)\n",
    "        # 使用CSS選擇器定位最後一個按鈕並點擊\n",
    "        if total_page >= 15:\n",
    "            k = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    button_element = WebDriverWait(driver, 4).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, '#js-job-content > div:last-child > button'))\n",
    "                    )\n",
    "                    print(f'手動載入第{15 + k}頁')\n",
    "                    button_element.click()\n",
    "                    k += 1\n",
    "                    if k == 86 or k == total_page - 14 :\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(\"發生未知錯誤：\", e)\n",
    "                    break\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        raw_Job_list = soup.find_all(\"a\",class_=\"js-job-link\")\n",
    "        print(f'共{len(raw_Job_list)}筆資料')\n",
    "        driver.quit()\n",
    "        return raw_Job_list\n",
    "    \n",
    "    def filter_job(self, raw_Job_list:list):\n",
    "        filter_job_list = [i for i in raw_Job_list if re.search(self.key_word, i['title'].lower())]\n",
    "        print(f'過濾完有{len(filter_job_list)}筆')\n",
    "        return filter_job_list\n",
    "    \n",
    "    def update_date(self, soup) -> str:\n",
    "        update_date = soup.find(\"div\", class_=\"job-header__title\")\n",
    "        return update_date.find('span').text.strip().replace('更新','') if update_date else '無'\n",
    "    \n",
    "    def company(self, soup) -> str:\n",
    "        name = soup.find(\"div\", class_=\"mt-3\")\n",
    "        return name.select_one('div > a').text.strip() if name else '無'\n",
    "    \n",
    "    def jd_info(self, soup) -> dict:\n",
    "        result = {}\n",
    "        JD = soup.find('div', class_='job-description-table row')\n",
    "        if JD:\n",
    "            jd_items = JD.find_all('div', recursive=False)\n",
    "            if jd_items:\n",
    "                    try:\n",
    "                        job_content = jd_items[0].find('p').text if jd_items[0].find('p').text else '無'\n",
    "                    except:\n",
    "                        job_content = '無'\n",
    "\n",
    "                    try:\n",
    "                        job_category = ', '.join(i.text for i in jd_items[1].find_all('u')) if jd_items[1].find_all('u') else '無'\n",
    "                    except:\n",
    "                        job_category = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        salary = jd_items[2].find_all('div', recursive=False)[-1].text.strip() if jd_items[2].find_all('div', recursive=False)[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        salary = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        job_type = jd_items[3].find_all('div')[-1].text.strip() if jd_items[3].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        job_type = '無'\n",
    "\n",
    "                    try:\n",
    "                        workingplace = jd_items[4].find_all('div')[-1].text.strip() if jd_items[4].find_all('div')[-1].text.strip() != '代企業徵才' else None\n",
    "                    except:\n",
    "                        workingplace = '無'\n",
    "\n",
    "                    try:\n",
    "                        management_responsibility = jd_items[6].find_all('div')[-1].text.strip() if jd_items[6].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        management_responsibility = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        business_trip = jd_items[7].find_all('div')[-1].text.strip() if jd_items[7].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        business_trip = '無'\n",
    "\n",
    "                    try:\n",
    "                        working_duration = jd_items[8].find_all('div')[-1].text.strip() if jd_items[8].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        working_duration = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        Holiday_System = jd_items[9].find_all('div')[-1].text.strip() if jd_items[9].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        Holiday_System = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        Working_date = jd_items[10].find_all('div')[-1].text.strip() if jd_items[10].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        Working_date = '無'\n",
    "                    \n",
    "                    try:\n",
    "                        ppl_required = jd_items[11].find_all('div')[-1].text.strip() if jd_items[11].find_all('div')[-1].text.strip() else '無'\n",
    "                    except:\n",
    "                        ppl_required = '無'\n",
    "\n",
    "\n",
    "                    result = {\n",
    "                                '工作內容': job_content,\n",
    "                                '職務類別': job_category,\n",
    "                                '工作待遇': salary,\n",
    "                                '工作性質': job_type,\n",
    "                                '上班地點': workingplace,\n",
    "                                '管理責任': management_responsibility,\n",
    "                                '出差外派': business_trip,\n",
    "                                '上班時段': working_duration,\n",
    "                                '休假制度': Holiday_System,\n",
    "                                '可上班日': Working_date,\n",
    "                                '需求人數': ppl_required\n",
    "                            }\n",
    "        return result\n",
    "    \n",
    "    def jr_info(self, soup) -> dict:\n",
    "        result = {}\n",
    "        JR = soup.find('div', class_= 'job-requirement-table row')\n",
    "        JRO = soup.find('div', class_= 'job-requirement col opened')\n",
    "        if JR:\n",
    "            jr_items = JR.find_all('div', recursive=False)\n",
    "            if jr_items:\n",
    "                try:\n",
    "                    work_exp = jr_items[0].find_all('div')[-1].text.strip() if jr_items[0].find_all('div')[-1].text.strip() else '無'\n",
    "                except:\n",
    "                    work_exp = '無'\n",
    "                \n",
    "                try:\n",
    "                    academic_require = jr_items[1].find_all('div')[-1].text.strip() if jr_items[1].find_all('div')[-1].text.strip() else '無'\n",
    "                except:\n",
    "                    academic_require = '無'\n",
    "                \n",
    "                try:\n",
    "                    department_require = jr_items[2].find_all('div')[-1].text.strip() if jr_items[2].find_all('div')[-1].text.strip() else '無'\n",
    "                except:\n",
    "                    department_require = '無'\n",
    "\n",
    "                try:\n",
    "                    language = jr_items[3].find('p').text.strip() if jr_items[3].find('p').text.strip() else '無'\n",
    "                except:\n",
    "                    language = '無'\n",
    "\n",
    "                try:\n",
    "                    tool = ', '.join(i.text for i in jr_items[4].find_all('u')) if jr_items[4].find_all('u') else '無'\n",
    "                except:\n",
    "                    tool = '無'\n",
    "                \n",
    "                try:\n",
    "                    working_ability = jr_items[5].find_all('div')[-1].text.strip() if jr_items[5].find_all('div')[-1].text.strip() else '無'\n",
    "                except:\n",
    "                    working_ability = '無'\n",
    "   \n",
    "        try:\n",
    "            others = JRO.find_all('div')[-1].text.strip() if JRO.find_all('div')[-1].text.strip() else '無'\n",
    "        except:\n",
    "            others = '無'\n",
    "\n",
    "        if JR and JRO:\n",
    "            result = {\n",
    "                '工作經歷' : work_exp,\n",
    "                '學歷要求' : academic_require,\n",
    "                '科系要求' : department_require,\n",
    "                '語文條件' : language,\n",
    "                '擅長工具' : tool,\n",
    "                '工作技能' : working_ability,\n",
    "                '其他要求' : others\n",
    "            }\n",
    "\n",
    "        # result = {\n",
    "        #     '工作經歷' : work_exp,\n",
    "        #     '學歷要求' : academic_require,\n",
    "        #     '科系要求' : department_require,\n",
    "        #     '語文條件' : language,\n",
    "        #     '擅長工具' : tool,\n",
    "        #     '工作技能' : working_ability,\n",
    "        #     '其他要求' : others\n",
    "        # }\n",
    "        return result\n",
    "    \n",
    "    async def fetch(self, session, url):\n",
    "        async with session.get(url, headers = {'User-Agent':'GoogleBot'}) as response:\n",
    "            return await response.text()\n",
    "\n",
    "    async def get_job_info(self, item):\n",
    "        try:\n",
    "            title = item['title']\n",
    "            Job_link = f\"https:{item['href']}\"\n",
    "            connector = TCPConnector(limit=10)\n",
    "            async with ClientSession(connector=connector) as session:\n",
    "                html = await self.fetch(session, Job_link)\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            Data = {\n",
    "                '更新日期': [self.update_date(soup)],\n",
    "                '職缺名稱': [title],\n",
    "                '公司名稱': [self.company(soup)],\n",
    "                '連結': [Job_link]\n",
    "            }\n",
    "            Data.update(self.jd_info(soup))\n",
    "            Data.update(self.jr_info(soup))\n",
    "            df = pd.DataFrame(Data, columns=['更新日期', '職缺名稱', '公司名稱', '工作內容', '職務類別', '工作待遇',\n",
    "                                            '工作性質', '上班地點', '管理責任', '出差外派', '上班時段', '休假制度',\n",
    "                                            '可上班日', '需求人數', '工作經歷', '學歷要求', '科系要求', '語文條件',\n",
    "                                            '擅長工具', '工作技能', '其他要求', '連結'])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(\"發生錯誤\", e)\n",
    "            return None\n",
    "\n",
    "    async def scrape(self, Job_list):\n",
    "        tasks = []\n",
    "        semaphore = asyncio.Semaphore(10)  # Limit concurrent requests to 10\n",
    "\n",
    "        for item in Job_list:\n",
    "            async with semaphore:\n",
    "                task = asyncio.ensure_future(self.get_job_info(item))\n",
    "                tasks.append(task)\n",
    "\n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "    def main(self, Job_list: list, DF):\n",
    "        print(len(Job_list))\n",
    "        batch_size = 30\n",
    "        num_batches = (len(Job_list) + batch_size - 1) // batch_size\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(Job_list))\n",
    "            Job_list_batch = Job_list[start_idx:end_idx]\n",
    "            loop = asyncio.get_event_loop()\n",
    "            results = loop.run_until_complete(self.scrape(Job_list_batch))\n",
    "            # loop.close()\n",
    "\n",
    "            for df in results:\n",
    "                if df is not None:\n",
    "                    DF = pd.concat([DF, df], ignore_index=True)\n",
    "\n",
    "        DF.to_csv(f\"./output/JBLIST_{self.current_date}.csv\", sep=',', index=False)\n",
    "        return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8874530-44d8-4fd4-a32f-dbad7ed74302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.104.com.tw/jobs/search/?ro=1&keyword=%E8%B3%87%E6%96%99%E5%B7%A5%E7%A8%8B&area=6001002000%2C6001001000%2C6001006000%2CC6001008000&isnew=3&jobexp=1%2C3&mode=l&order=16\n",
      "Total_page = 65\n",
      "手動載入第16頁\n",
      "手動載入第17頁\n",
      "手動載入第18頁\n",
      "手動載入第19頁\n",
      "手動載入第20頁\n",
      "手動載入第21頁\n",
      "手動載入第22頁\n",
      "手動載入第23頁\n",
      "手動載入第24頁\n",
      "手動載入第25頁\n",
      "手動載入第26頁\n",
      "手動載入第27頁\n",
      "手動載入第28頁\n",
      "手動載入第29頁\n",
      "手動載入第30頁\n",
      "手動載入第31頁\n",
      "手動載入第32頁\n",
      "手動載入第33頁\n",
      "手動載入第34頁\n",
      "手動載入第35頁\n",
      "手動載入第36頁\n",
      "手動載入第37頁\n",
      "手動載入第38頁\n",
      "手動載入第39頁\n",
      "手動載入第40頁\n",
      "手動載入第41頁\n",
      "手動載入第42頁\n",
      "手動載入第43頁\n",
      "手動載入第44頁\n",
      "手動載入第45頁\n",
      "手動載入第46頁\n",
      "手動載入第47頁\n",
      "手動載入第48頁\n",
      "手動載入第49頁\n",
      "手動載入第50頁\n",
      "手動載入第51頁\n",
      "手動載入第52頁\n",
      "手動載入第53頁\n",
      "手動載入第54頁\n",
      "手動載入第55頁\n",
      "手動載入第56頁\n",
      "手動載入第57頁\n",
      "手動載入第58頁\n",
      "手動載入第59頁\n",
      "手動載入第60頁\n",
      "手動載入第61頁\n",
      "手動載入第62頁\n",
      "手動載入第63頁\n",
      "手動載入第64頁\n",
      "手動載入第65頁\n",
      "共1930筆資料\n",
      "過濾完有1271筆\n",
      "1271\n",
      "花費 143.77451586723328 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 過濾關鍵字以外的職缺\n",
    "keywords_pattern = r'工程|資料|python|data|數據'\n",
    "\n",
    "# 搜尋關鍵字\n",
    "filter_params = {\n",
    "    'ro' : 1, # 1 全職\n",
    "    'keyword' : '資料工程',\n",
    "    'area' : '6001002000,6001001000,6001006000,C6001008000',  # 6001001000 台北市 6001002000 新北 6001006000 新竹縣市 6001008000 台中市\n",
    "    'isnew' : 3, # 0:本日 3:3天內 7:1週內 14 30\n",
    "    'jobexp' : '1,3', # 工作經驗1年以下 + 1-3年\n",
    "    'mode' : 'l', # 列表模式(比較多筆資料)\n",
    "    'order' : 16 # 照日期排序\n",
    "}\n",
    "\n",
    "# 建立物件\n",
    "DF = pd.DataFrame()\n",
    "EJS = eJob_search104(filter_params, keywords_pattern)\n",
    "retry_count = 0\n",
    "while True:\n",
    "    try:\n",
    "        raw_Job_list = EJS.search_job()\n",
    "        break\n",
    "    except:\n",
    "        retry_count += 1\n",
    "        if retry_count == 3:\n",
    "            break\n",
    "        print(f'執行錯誤, retry {retry_count}')\n",
    "Job_list = EJS.filter_job(raw_Job_list)\n",
    "result_df = EJS.main(Job_list, DF)\n",
    "print(f\"花費 {time.time() - start_time} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504eba65-b520-4ac1-ad2c-556e6af7615a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
